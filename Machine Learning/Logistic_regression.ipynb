{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "name": "Logistic_regression.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftyQdMamGiib"
      },
      "source": [
        "import numpy as np\n",
        "def data(n, m, theta):\n",
        "    beta = np.random.randn(m+1,1)\n",
        "    X = np.random.randn(n, m)\n",
        "    x0 = np.ones((n,1))\n",
        "    X = np.hstack((x0,X))\n",
        "    \n",
        "    exp = 1/(1+np.exp(-(np.dot(X, beta)))) \n",
        "    # print(exp)\n",
        "    noise = np.random.binomial(1, theta, n)\n",
        "    noise = np.reshape(noise, (n,1))\n",
        "    # print(\"Hi Im Noise \\n\",noise)\n",
        "    exp_noise = exp+noise\n",
        "    # print(\"Hi I'm Exp Noise \\n\",exp_noise)\n",
        "    y = np.where(exp_noise > .5, 1, 0) \n",
        "    return X, y, beta"
      ],
      "id": "ftyQdMamGiib",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlnWdYSNGiic"
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1/(1+ np.exp(-z))"
      ],
      "id": "RlnWdYSNGiic",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N51jPScuGiid"
      },
      "source": [
        "def loss(pred_y, y):\n",
        "    return (-y.T*np.log(pred_y) - (1-y).T*np.log(1 - pred_y)).mean()"
      ],
      "id": "N51jPScuGiid",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIlKcQBQGiid"
      },
      "source": [
        "def gradient(X, y, pred_y):\n",
        "    return (np.matmul(X.T,(pred_y - y)))/y.shape[0]"
      ],
      "id": "sIlKcQBQGiid",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrdCWsESGiid"
      },
      "source": [
        "def logistic(X, y, k, threshold, LR):\n",
        "    rows = X.shape[0]\n",
        "    colum = X.shape[1]\n",
        "    \n",
        "    beta = np.random.rand(colum,1)\n",
        "    init_cost = float(\"inf\")\n",
        "    for _ in range(k):\n",
        "        z = np.matmul(X, beta)   \n",
        "        pred_y = sigmoid(z)      \n",
        "        cost = loss(pred_y, y)\n",
        "        if abs(init_cost - cost) <= threshold:\n",
        "            break\n",
        "        else:\n",
        "            init_cost=cost\n",
        "            grad = gradient(X,y,pred_y)\n",
        "            beta=beta - (float(LR)*grad)\n",
        "    return init_cost,beta"
      ],
      "id": "xrdCWsESGiid",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21OhA1rzGiie"
      },
      "source": [
        "X,y,w=data(10,10,0.1)\n",
        "print(y)"
      ],
      "id": "21OhA1rzGiie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDicLp2uGiif",
        "outputId": "aab18381-8772-40db-d32d-1eadc5acc43c"
      },
      "source": [
        "cost,s = logistic(X,y,20000,0.0000004,.001)\n",
        "print(cost)"
      ],
      "id": "rDicLp2uGiif",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7114323029388264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djxk8L1WGiig"
      },
      "source": [
        "def loss_l1(pred_y, y, n, lmda, beta):\n",
        "    return (-y.T*np.log(pred_y) - (1-y).T*np.log(1 - pred_y)).mean() + (lmda/n)*np.sum(np.absolute(beta))\n",
        "\n",
        "def gradient_l1(X,y,pred_y,n,lmda,beta):\n",
        "    g_0 = np.array(np.mean(pred_y-y)).reshape(1,1)\n",
        "    new_lmda = lmda*(np.absolute(beta)/beta) \n",
        "    g_1 = (np.matmul(X.T[1:],(pred_y-y))/n) +(new_lmda/n)\n",
        "    grad = np.concatenate((g_0,g_1),axis=0) \n",
        "    return grad"
      ],
      "id": "djxk8L1WGiig",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BZACElFGiig"
      },
      "source": [
        "def logistic_with_l1(X, y, k, threshold, LR, lmda):\n",
        "    rows = X.shape[0]\n",
        "    colum = X.shape[1]\n",
        "    \n",
        "    beta = np.random.rand(colum,1)\n",
        "    init_cost = float(\"inf\")\n",
        "    for _ in range(k):\n",
        "        z = np.matmul(X, beta)            \n",
        "        pred_y = sigmoid(z)               \n",
        "        cost = loss_l1(pred_y, y, rows, lmda, beta[1:])\n",
        "        if abs(init_cost - cost) <= threshold:\n",
        "            break\n",
        "        else:\n",
        "            init_cost=cost\n",
        "            grad = gradient_l1(X,y,pred_y, rows, lmda, beta[1:])\n",
        "            beta=beta - (float(LR)*grad)\n",
        "    return init_cost,beta"
      ],
      "id": "2BZACElFGiig",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BTA9AyjGiig",
        "outputId": "f96ea018-d0a0-4da2-9853-1d3c562d206d"
      },
      "source": [
        "cost,s = logistic_with_l1(X,y,10000,0.0000005,.001,0.7)\n",
        "print(cost)"
      ],
      "id": "7BTA9AyjGiig",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8477136334983164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gojcjoK4Giih"
      },
      "source": [
        "def loss_l2(pred_y, y, n, lmda, beta):\n",
        "    return (-y.T*np.log(pred_y) - (1-y).T*np.log(1 - pred_y)).mean() + (lmda/n)*np.sum(beta*beta)\n",
        "\n",
        "def gradient_l2(X,y,pred_y,n,lmda,beta):\n",
        "    g_0 = np.array(np.mean(pred_y-y)).reshape(1,1)  \n",
        "    g_1 = (np.matmul(X.T[1:],(pred_y-y))/n) + 2*(lmda*beta)/n              \n",
        "    grad = np.concatenate((g_0,g_1),axis=0) \n",
        "    return grad\n",
        "\n",
        "def logistic_with_l2(X, y, k, threshold, LR, lmda):\n",
        "    rows = X.shape[0]\n",
        "    colum = X.shape[1]\n",
        "    \n",
        "    beta = np.random.rand(colum,1)\n",
        "    init_cost = float(\"inf\")\n",
        "    for _ in range(k):\n",
        "        z = np.matmul(X, beta)         \n",
        "        pred_y = sigmoid(z)            \n",
        "        cost = loss_l2(pred_y, y, rows, lmda, beta[1:])\n",
        "        if abs(init_cost - cost) <= threshold:\n",
        "            break\n",
        "        else:\n",
        "            init_cost=cost   \n",
        "            grad = gradient_l2(X,y,pred_y, rows, lmda, beta[1:])\n",
        "            beta=beta - (float(LR)*grad)\n",
        "    return init_cost,beta"
      ],
      "id": "gojcjoK4Giih",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ils6K_oGGiii",
        "outputId": "f9cb828c-68d6-4772-dffa-76a3620fef9f"
      },
      "source": [
        "cost,s=logistic_with_l2(X,y,10000,0.0000005,.001,0.7)\n",
        "print(cost)"
      ],
      "id": "Ils6K_oGGiii",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7671850804154664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoFu-8rFGiii"
      },
      "source": [
        "import numpy as np\n",
        "class Regression:\n",
        "    def __init__(self, modal_type, regulizer=None, lmda=None):\n",
        "        self.modal_type = modal_type\n",
        "        self.regulizer = regulizer\n",
        "        self.lmda = lmda\n",
        "    \n",
        "    def data(self, n, m, sigma=None, theta=None):\n",
        "        if(self.modal_type==\"Linear\"):\n",
        "            e = np.random.normal(0, sigma, n)  \n",
        "            e = np.reshape(e, (n,1)) \n",
        "            beta=np.random.rand(m+1,1)    \n",
        " \n",
        "            X = np.random.rand(n, m+1)    \n",
        "            X[:, 0] = 1                  \n",
        "            y=np.matmul(X,beta)+e          \n",
        "            return X, y, beta\n",
        "        \n",
        "        elif(self.modal_type==\"Logistic\"):\n",
        "   \n",
        "            beta = np.random.randn(m+1,1)\n",
        "            \n",
        "            X = np.random.randn(n, m)\n",
        "            x0 = np.ones((n,1))\n",
        "            X = np.hstack((x0,X))\n",
        "            exp = 1/(1+np.exp(-(np.dot(X, beta))))         \n",
        "            noise = np.random.binomial(1, theta, n)   \n",
        "            noise = np.reshape(noise, (n,1))\n",
        "            exp_noise = exp+noise\n",
        "            y = np.where(exp_noise > .5, 1, 0)\n",
        "\n",
        "            return X, y, beta\n",
        "        else:\n",
        "            print(\"choose modal_type: Linear, Logistic\")\n",
        "            \n",
        "    def regression(self, X, y,k, threshold):\n",
        "        learning_rate = 0.001\n",
        "\n",
        "        n = X.shape[0]\n",
        "        m = X.shape[1]\n",
        "\n",
        "        weight = np.random.rand(m,1) * learning_rate\n",
        "\n",
        "        initial_cost = float('inf') \n",
        "        for _ in range(k):\n",
        "            y_predicted = np.matmul(X, weight)\n",
        "            cost = (1/n) * np.sum((y_predicted - y)**2)  \n",
        "            if abs(initial_cost - cost) <= threshold:\n",
        "                break\n",
        "            initial_cost = cost             \n",
        "            gd = (-2/n) * np.matmul((y - y_predicted).transpose(),X)   \n",
        "            weight = weight - learning_rate * gd    \n",
        "        return initial_cost, weight\n",
        "    \n",
        "    def sigmoid(self, z):\n",
        "        return 1/(1+ np.exp(-z))\n",
        "    \n",
        "    def loss(self, pred_y, y):\n",
        "        return (-y.T*np.log(pred_y) - (1-y).T*np.log(1 - pred_y)).mean()\n",
        "    \n",
        "    def gradient(self, X, y, pred_y):\n",
        "        return (np.matmul(X.T,(pred_y - y)))/y.shape[0]\n",
        "    \n",
        "\n",
        "    def loss_l1(self, pred_y, y, n, lmda, beta):\n",
        "        return (-y.T*np.log(pred_y) - (1-y).T*np.log(1 - pred_y)).mean() + (lmda/n)*np.sum(abs(beta))\n",
        "\n",
        "    def gradient_l1(self, X,y,pred_y,n,lmda,beta):\n",
        "        g_0 = np.array(np.mean(pred_y-y)).reshape(1,1)\n",
        "        new_lmda = lmda*(np.absolute(beta)/beta) \n",
        "        g_1 = np.matmul(X.T[1:],(pred_y-y))/n +(new_lmda/n)\n",
        "        grad = np.concatenate((g_0,g_1),axis=0)    \n",
        "        return grad\n",
        "        \n",
        "    def loss_l2(self, pred_y, y, n, lmda, beta):\n",
        "        return (-y.T*np.log(pred_y) - (1-y).T*np.log(1 - pred_y)).mean() + (lmda/(2*n))*np.sum(beta*beta)\n",
        "\n",
        "    def gradient_l2(self, X, y, pred_y, n, lmda, beta):\n",
        "        g_0 = np.array(np.mean(pred_y-y)).reshape(1,1)  \n",
        "        g_1 = np.matmul(X.T[1:],(pred_y-y))/n + (lmda*beta)/n              \n",
        "        grad = np.concatenate((g_0,g_1),axis=0)    \n",
        "        return grad\n",
        "\n",
        "    def logistic(self, X, y, k, threshold, LR):\n",
        "        rows = X.shape[0]\n",
        "        colum = X.shape[1]\n",
        "    \n",
        "        beta = np.random.rand(colum,1)\n",
        "        init_cost = float(\"inf\")\n",
        "        for _ in range(k):\n",
        "            z = np.matmul(X, beta)              \n",
        "            pred_y = self.sigmoid(z)                  \n",
        "            if self.regulizer=='l1':\n",
        "                cost = self.loss_l1(pred_y, y, rows, self.lmda, beta[1:])\n",
        "            elif self.regulizer=='l2':\n",
        "                cost = self.loss_l2(pred_y, y, rows, self.lmda, beta[1:])\n",
        "            else:\n",
        "                cost = self.loss(pred_y, y)\n",
        "            if abs(init_cost - cost) <= threshold:\n",
        "                break\n",
        "            else:\n",
        "                init_cost=cost\n",
        "                if self.regulizer=='l1':\n",
        "                    grad = self.gradient_L1(X,y,pred_y, rows, self.lmda, beta[1:])\n",
        "                elif self.regulizer=='l2':\n",
        "                    grad = self.gradient_l2(X,y,pred_y, rows, self.lmda, beta[1:])\n",
        "                else:\n",
        "                    grad = self.gradient(X,y,pred_y)\n",
        "                beta = beta - (float(LR)*grad)\n",
        "        return init_cost,beta"
      ],
      "id": "hoFu-8rFGiii",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9WJZgiIGiij"
      },
      "source": [
        "modal = Regression(modal_type=\"Logistic\")"
      ],
      "id": "M9WJZgiIGiij",
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZMpJJGmGiij"
      },
      "source": [
        "X, y, beta = modal.data(4,4,theta=0.7)"
      ],
      "id": "2ZMpJJGmGiij",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-kjXj7LGiik",
        "outputId": "c9987e31-5db9-4f26-d284-7f73bb91613c"
      },
      "source": [
        "cost,s = modal.logistic(X, y, 10000, 0.0000005, .001)\n",
        "print(cost)"
      ],
      "id": "5-kjXj7LGiik",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8251207999963981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74mpmz2aGiil",
        "outputId": "57716526-7660-4ed7-c1ec-f0de86cc0a81"
      },
      "source": [
        "modal = Regression(modal_type=\"Logistic\", regulizer=\"L1\", lmda=0.7)\n",
        "cost, s = modal.logistic(X, y, 10000, 0.0000005, .001)\n",
        "print(cost)"
      ],
      "id": "74mpmz2aGiil",
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7827304365063905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM0R5OkyGiil",
        "outputId": "d5ea58f5-5492-41ac-a05d-638fe103f7a5"
      },
      "source": [
        "modal = Regression(modal_type=\"Logistic\", regulizer=\"l2\", lmda=0.7)\n",
        "cost,s = modal.logistic(X, y, 10000, 0.0000005, .001)\n",
        "print(cost)"
      ],
      "id": "YM0R5OkyGiil",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7809364656118876\n"
          ]
        }
      ]
    }
  ]
}